---
title: "GeoAdjust : Adjusting for Positional Uncertainty in DHS Household Surveys"
output: rmarkdown::html_vignette
bibliography: /Users/umuta/GeoAdjust/references.bib
vignette: >
  %\VignetteIndexEntry{GeoAdjust}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  
  header-includes:
    - \usepackage{hyperref}y
---

<style>
body {
text-align: justify}
</style>



```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, setup}
library(GeoAdjust)
```

## ABSTRACT
The Demographic and Health Surveys Program (DHS) conducts household surveys in low- and middle-income countries. The surveys are designed as the stratified cluster sampling, where each household cluster is located within either urban or rural stratum. DHS applies a number of measures in order to make sure that the privacy of the survey participants is protected. These measures involve first aggregating the collected data into the GPS coordinates of the household cluster centers, and then randomly diplacing the true locations of cluster centers with respect to a known jittering algorithm. The random displacement leads the DHS coordinates to contain positional error. We recently discovered that the unaddressed positional error results in the model parameter estimates that are attenuated towards zero together with a poorer predictive performance [@altay2022accounting], [@altay2022covariates]. GeoAdjust package is constructed in order to provide fast computational tools which allow accounting/adjusting for positional uncertainty in the GPS coordinates of the observed data. The package makes use of the methods that are developed and discussed in detail in [@altay2022accounting] and [@altay2022covariates]. 

## INTRODUCTION
DHS jittering algorithm is applied by choosing a uniform random angle between 0 and $2\pi$ and shifting the true location up to a certain maximum jittering distance towards the corresponding direction.  The maximum jittering distances are decided based on the strata that the cluster center is located within.The urban cluster centers are displaced up to 2 kilometers while the rural ones are jittered up to 5 kilometers, with %1 of them being jittered up to 10 kilometers [@DHSspatial07]. 

The jittering is done by respecting the administrative area borders, meaning that the jittered location is not allowed to end up in an administrative area that is different than the initial one. Jittering is repeated until this condition is satisfied. Administrative areas are classified into different hierarchical levels starting from the national (or admin0) level. National level refers to the entire country which is seperated from its neighbours via the national or admin0 level borders. The classification follows hierarchically through smaller subnational levels as counties, provinces, etc., which are also coded by the increasing numbers such as admin1, admin2, etc. 

It is important to be aware that the jittering can be applied with respect to different admin levels in different countries. The information about this can be found in the corresponding README.txt (it is called "GPS_Displacement_README.txt" for the Nigeria 2018 survey, might be different elsewhere) file of the corresponding DHS data set. 



### Data Structure and the Package Workflow

DHS data set consists of both individual level and cluster level information. The individual level data is a Stata file mainly containing the responses of the participants to the survey. The file has a variable referring to the corresponding cluster ID of each data row. The cluster level data is a shape file. It contains cluster identification information such as their jittered latitudes and longitudes (in degrees), cluster ID, corresponding strata ("R" for rural and "U" for urban) and the name of the administrative area that they are located within. Since the cluster ID is a common variable in both files, it will be useful while merging them into a single data frame. 

The analysis method requires two shape files. One of them is the shape file that contains the national (admin0) level borders of the country of interest. The second one should contain the borders that correspond to the administrative level areas which are respected while the true cluster centers are jittered. Shape files of various administrative levels for various countries are available at the  website of "the Database of Global Administrative Areas (GADM)" (https://gadm.org/data.html). Once downloaded, the files can be read into R as "SpatialPolygonsDataFrame" objects.

Finally, the covariates should be read into R as the raster layers. Accordingly, the main input is composed of the geographic data, survey data, covariate rasters and two shape files representing the corresponding national and subnational administrative level borders of the country of interest. The data files are processed with the functions provided with the package to produce output, namely, the model parameter estimates and the predictions on a prediction grid. The following figure shows the workflow through the functions of Geoadjust R-package:

![Workflow of GeoAdjust R-package](/Users/umuta/GeoAdjust/vignettes/workflow.png){ width=100% }


### Access to DHS Data

DHS data sets are not openly available to the public. Access to relevant DHS survey data sets requires applying for permission. One needs to briefly and clearly explain why the data is needed and how it will be processed and used, within a project summary framework. Sharing the data sets with the collaborating researchers requires permission as well. Step by step explanation about the application process can be found in DHS website (https://dhsprogram.com/data/new-user-registration.cfm). Once the permission is granted, the applicant receives a letter via email, which clearly states the content of the permission.


<!-- The methods included in GeoAdjust package utilize the auto-differentiation in Template Model Builder (TMB) R package. The approach involves creating a set of integration points within equidistant rings around each cluster center. Each cluster center is taken as the first integration point of the corresponding point set, and then depending on the urbanicity of the cluster, different number of rings are created together with their own sets of integration points. This approach allows us to consider the unknown true locations as the nuisance parameters in the model and then integrate them out by utilizing the auto differentiation of the Template Model builder (TMB) R-package. TMB operates via a C++ script which  contains the joint negative log-likelihood function and allows computing the contribution of each integration point to the joint negative log-likelihood. -->

<!-- This package aims to provide fast and useful tools that allow doing inference and prediction while adjusting/accounting for the positional error in the GPS coordinates of the survey cluster centers. -->

## The Adjusted Model

The method behind GeoAdjust R-package aims for adjusting for the adverse effects of jittering on the inference and the package is the end product of the development process of the corresponding approach The method is discussed in detail in [@altay2022accounting] and [@altay2022covariates]. The core idea is treating the unknown true locations as the nuisance parameters and integrating them out of the joint likelihood function. In practice, this is applied by utilizing the auto-differentiation feature of the Template Model Builder (TMB) R-package. 

We call the true GPS coordinates of the centroids of each household cluster as the "true locations", whereas the published GPS coordinates of the cluster centers are referred as the "observed locations". Let $\boldsymbol{s}_1^*, \ldots, \boldsymbol{s}_C^* \in\mathcal{D}$ denote the true locations of the corresponding published DHS cluster centers $\boldsymbol{s}_1, \ldots, \boldsymbol{s}_C$. Adjusting for jittering within the binomial modelling framework yields the following observation model:
\begin{align}
\label{eqn:adjObsModelBinomial}
\begin{split}
y_c | r_c,n_c &\sim \text{Binomial}(n_c, r_c), \quad \boldsymbol{s}_c|\boldsymbol{s}_c^*\sim \pi_{\mathrm{Urb}[c]}(\boldsymbol{s}_c|\boldsymbol{s}_c^*),\\
    r_c | \boldsymbol{s}_c^* &= r(\boldsymbol{s}_c^*) = \mathrm{logit}^{-1}( \eta(\boldsymbol{s}_c^*)),
\end{split}
\end{align}

where $r_c$ denotes the risk of interest in cluster $c$, and $\mathrm{Urb}[c]\in\{\mathrm{U}, \mathrm{R}\}$ corresponds to the urban (U) or rural (R) stratum that cluster c is located within. In this observation model, both $y_c$ and $\boldsymbol{s}_c$ are already available through the DHS data set and accordingly, treated as the observed quantities. The unobserved true locations $s_c^*$ on the other hand, are inaccessible. They are treated as the random quantities and assigned a uniform prior $s_c^*\sim \mathcal{U}(\mathcal{D})$. This implies that we treat all locations $\boldsymbol{s}_c^*$ within the maximum jittering distance from $\boldsymbol{s}_c$ as equally likely \emph{a priori}, meaning that the true location can be located anywhere within the corresponding jittering radius with equal probability.

If the interest is on modelling the Poisson counts, then the adjusted model becomes:

\begin{align}
\label{eqn:adjObsModelPoisson}
\begin{split}
y_c | \lambda_c &\sim \text{Poisson}(\lambda_c), \quad \boldsymbol{s}_c|\boldsymbol{s}_c^*\sim \pi_{\mathrm{Urb}[c]}(\boldsymbol{s}_c|\boldsymbol{s}_c^*),\\
    log(\lambda_c) | \boldsymbol{s}_c^* &=  \eta(\boldsymbol{s}_c^*),
\end{split}
\end{align}
where, $\lambda_c$ are the Poisson counts of interest.

Similarly, the following represents the adjusted Gaussian model:
\begin{align*}
{\mathrm{y}_\mathrm{c} \mid \mathrm{\eta}_\mathrm{\boldsymbol{s}_c^*}, \sigma_\epsilon^2 \sim \mathcal{\mathrm{\mathcal{N}}}(\eta_\mathrm{\boldsymbol{s}_c^*}, \sigma_\epsilon^2}).
\end{align*}

\begin{align*}
\mathrm{\eta}(\mathrm{\boldsymbol{s}_c^*})=\mu+\mathrm{X}(\mathrm{\boldsymbol{s}_c^*})^T\beta+\mathrm{u}(\mathrm{\boldsymbol{s}_c^*}).
\end{align*}
where $\sigma_\epsilon^2$ is the nugget variance, $\mu$ is the intercept, $X$ is the design matrix and $\beta$ is a vector of covariate effect sizes.

In practice, estimating these different models using GeoAdjust R-package will involve passing the relevant model components as the arguments of the corresponding functions, which will be explained in more detail within next sections.  

### The SPDE Approach

We model the latent spatial part $u(\cdot)$ of the observation model as a Gaussian Random Field with a Matern covariance function. The stochastic partial differential equations (SPDE) approach [@Lindgren:etal:11] provides an approximation to the MatÃ©rn GRF that results in a sparse precision matrix. This is applied by first triangulating the area of interest through $m$ nodes and then approximating the GRF $u(\cdot)$ by:

\begin{equation}
\tilde{u}(\boldsymbol{s}) = \sum_{i=1}^m w_i \phi_i(s),\label{eq:SPDE:basis}
\end{equation}
where $\phi_i(\cdot)$ are pyramidal basis functions and  $\boldsymbol{w} = (w_1 \ \ldots \ w_m)^\mathrm{T}$
are weights for the basis functions. The SPDE approach results in a distribution $\boldsymbol{w}\sim\mathcal{N}_m(\boldsymbol{0}, \mathbf{Q}(\boldsymbol{\theta})^{-1})$, where $\mathbf{Q}(\boldsymbol{\theta})$ is sparse.


## Data Preparation
Assume that we want to estimate the risk of completing secondary education among 20-39 years old women throughout Nigeria. Although it seems straight forward, it still might be time saving for the potential users of the package to see an example regarding how the data is extracted from the main input files and processed towards forming a main input data list. It is not possible to share DHS data publicly, therefore the real survey data will not be processed through this manuscript. Instead, an example data set mimicing the DHS data will be used. Similar to the Nigeria DHS 2018 household survey (NDHS-2018) data files, the example data set consist of a Stata file and a shape file. In order to be consistent with NDHS-2018 survey, the example data is constructed using the same variable names (v001, v012, etc.). Detailed description of the variables can be found in [@fund2018demographic]. Following is the demonstration for first extracting and then merging the individual and cluster level data, based on the Nigeria DHS 2018 (NDHS-2018) household survey file and variable names:

Reading the shape files of administrative area borders:

```{r eval=FALSE}
# the shape files corresponding to the admin0 and admin2 borders :
library(rgdal)
admin0 = readOGR(dsn = "dataFiles/gadm40_NGA_shp",
                            layer = "gadm40_NGA_0")

admin2 = readOGR(dsn = "dataFiles/gadm40_NGA_shp",
                     layer = "gadm40_NGA_2")

admin2 = admin2[-160,] # removing the lake (contained in polygon 160)
```

Reading the DHS files:

```{r eval = FALSE}
library(haven)
library(rgdal)
# find the path to the example files:
path1 = system.file("extdata", "individualLevel.DTA", package = "GeoAdjust", mustWork = TRUE)

# path2 is the path of the folder that contains the .shp file
path2 = system.file("extdata", "clusterLevel", package = "GeoAdjust", mustWork = TRUE)

# individual level data (survey responses) :
educationData = read_dta(path1) 

# cluster level data (clusterID, cluster center coordinates, strata, etc.)
corList = readOGR(dsn = path2,layer = "clusterLevel")
```

Extracting data:

```{r eval = FALSE}
# extract cluster level information:
smallGeo = data.frame(clusterIdx = corList$DHSCLUST, urban = corList$URBAN_RURA,
                      long = as.vector(corList@coords[,1]), lat = as.vector(corList@coords[,2]),
                      admin1 = corList$ADM1NAME)

#  extract individual level information:
myData = data.frame(clusterIdx = educationData$v001,         # ID numbers of the clusters
                    age = educationData$v012,                # age of the individual
                    secondaryEducation = educationData$v106) # v106 : highest education level attended
                                                             #    0 : no education
                                                             #    1 : primary
                                                             #    2 : secondary
                                                             #   >2 : higher
```


Subsetting and merging data:
```{r eval = FALSE}
# subset data with respect to the age interval of interest:
myData = subset(myData, age <= 39 & age >=20)

# number of 20-39 years old women who completed secondary education in each household
myData$ys = (myData$secondaryEducation>=2)+0

# merge the cluster level data with the subsetted individual level data,
# with respect to the cluster ID:
myData = merge(myData, smallGeo, by = "clusterIdx")

# add number of trials (for binomial response)
myData$Ntrials = 1

# aggregate the survey responses to the cluster centers
answers_x = aggregate(myData$ys,
                      by = list(clusterID = myData[, 1]),
                      FUN = sum)

answers_n= aggregate(myData$ys,
                     by = list(clusterID = myData[, 1]),
                     FUN = length)

# merge
answers_joint = merge(answers_x, answers_n, by="clusterID")

# now we have the total number of women participants within the relevant age interval (ns), 
# for each cluster. We also have the number of women among those who completed their secondary education (ys)
colnames(answers_joint) = c("clusterID", "ys", "ns")
```

Collecting everything into a main data frame:
```{r eval = FALSE}
# initial data frame
nigeria.data = data.frame(clusterID = corList@data[["DHSCLUST"]], long = as.vector(corList@coords[,1]), lat = as.vector(corList@coords[,2]))

# add ys and ns
nigeria.data = merge(nigeria.data, answers_joint, by="clusterID", all=T)

# add strata:
nigeria.data$urbanRuralDHS = corList@data[["URBAN_RURA"]]

# add coordinates in kilometers
nigeria.data$east = rep(NA, length(nigeria.data$long))
nigeria.data$north = rep(NA, length(nigeria.data$long)) 

nigeria.data[,c("east", "north")] = convertDegToKM(nigeria.data[,c("long", "lat")])

# remove non matching first row
nigeria.data = nigeria.data[-1,]
```

Fine tuning:
```{r eval = FALSE}
# jittering is done by respecting admin2 borders in Nigeria. 
# see if there are  cluster centers that doesn't match with any of the admin2 areas:

# first, add polygon IDs (some shape files may have it already) :
admin2@data[["OBJECTID"]] =1:774 # normally 775, we removed one (the lake)
                                 
# the cluster coordinates:
latLon = cbind(nigeria.data[,"long"], nigeria.data[,"lat"])
colnames(latLon) = c("long", "lat")

# make a SpatialPoints object
latLon = SpatialPoints(latLon, proj4string=CRS("+proj=longlat +datum=WGS84 +no_defs"), bbox = NULL)

# see if the points (cluster centers) are within the polygons (admin2 areas) :
check1 <- over(latLon, admin2, returnList = FALSE)

# drop the rows which don't match with none of the admin2 areas. 
# we will need them to match while creating the integration points later on.

# see which ones don't return a match :
which(is.na(check1$NAME_2))
# integer(0)


# This might not always be the case. If there were some non-matching rows,
# then they would be dropped as follows:

# which(is.na(check1$NAME_2))  # see the rows that don't match:
# [1]   48  122  205  848  857 1116 1122 1287 1328

# drop the corresponding rows from the main data set :
#nigeria.data = nigeria.data[-c(48, 122,  205,  848,  857, 1116, 1122, 1287, 1328),]
```

At this point the data frame `nigeria.data` should contain the variables that will be needed further in the analysis:
```{r eval = FALSE}
head(nigeria.data)
  clusterID      long       lat ys ns urbanRuralDHS      east     north
2         2  5.026532 13.432205  6 13             U -3386.003 1778.4821
3         3  5.083481  9.119869 10 22             U -3453.243 1211.9216
4         4 12.741408 12.668730 11 22             U -2446.272 1556.6579
5         5  4.155791  7.600494 18 25             U -3596.095 1022.3455
6         6  9.563798 11.676503 10 16             R -2846.535 1477.0920
7         7  8.025037  7.104230  5 10             U -3095.475  915.2284
```


#### Triangulation - The Mesh

We will construct the triangulation mesh based on the admin0 borders (in form of a SpatialPolygonsDataFrame object) of the country of interest. It is possible to manipulate the mesh resolution by setting the arguments max.edge and offset accordingly. 

```{r eval=FALSE}
library(rgdal)
library(ggplot2)
admin2 = readOGR(dsn = "dataFiles/gadm40_NGA_shp",
                     layer = "gadm40_NGA_2")

proj = "+units=km +proj=utm +zone=37 +ellps=clrk80 +towgs84=-160,-6,-302,0,0,0,0 +no_defs"

admin2@data[["OBJECTID"]] =1:775 # we now include the lake which is polygon number 160
admin2_trnsfrmd = spTransform(admin2,proj)
dfNigeria <- fortify(admin2_trnsfrmd, region = "NAME_2")

ggplot() + 
  geom_path(data = dfNigeria, aes(long,lat, group = group),colour = "black", inherit.aes = FALSE)+ theme_bw()+
  xlab("Easting (km)") + 
  ylab("Northing (km)")  +
  theme(panel.grid.major.x = element_blank(),
  panel.grid.minor.x = element_blank(),
  panel.grid.major.y = element_blank(),
  panel.grid.minor.y = element_blank()) +
  coord_fixed() +
  guides(fill = guide_colourbar(barwidth = 2.5, barheight = 25, title = labs(""), title.vjust=3) )+
  theme(legend.title = element_text(color = "transparent"), legend.text = element_text(color = "transparent"))+    
  scale_x_continuous(expand=c(0,0)) + scale_y_continuous(expand=c(0,0)) 

```

![admin2 level borders](/Users/umuta/GeoAdjust/vignettes/countryMap.png){ width=100% }



```{r eval=FALSE}
# since the coordinate system that we work on is in kilometers, 
# we need to transform admin0 file into kilometers first:

proj = "+units=km +proj=utm +zone=37 +ellps=clrk80 +towgs84=-160,-6,-302,0,0,0,0 +no_defs"
admin0_trnsfrmd = spTransform(admin0,proj)

mesh.s = meshCountry(admin0 = admin0_trnsfrmd, max.edge = c(25, 50), offset=-.08)
```
It might be useful to visually check by plotting the mesh and the country borders together:
```{r eval=FALSE}
library(inlabru)
ggplot() +
gg(mesh.s)  + theme_bw() +
coord_fixed() +
xlab("Easting (km)") + ylab("Northing (km)")  + 
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_blank(),
panel.grid.minor.y = element_blank())

```
![Mesh and the admin0 borders (blue)](/Users/umuta/GeoAdjust/vignettes/mesh.png){ width=100% }

## ESTIMATING MODEL PARAMETERS
We constructed our method of accounting/adjusting for positional uncertainty based on approaching the unknown true locations as nuisance parameters and integrating them out of the joint maximum likelihood function. This can be seen more clearly by writing the observation model as follows:

\begin{align}
    \pi(y_c, \boldsymbol{s}_c|\eta(\cdot)) &= \int_{\mathbb{R}^2} \pi(y_c, \boldsymbol{s}_c| \eta(\cdot), \boldsymbol{s}_c^*) \pi(\boldsymbol{s}_c^*) \ \mathrm{d}\boldsymbol{s}_c^* \notag \\
    &= \int_{\mathbb{R}^2} \pi(y_c| \eta(\boldsymbol{s}_c^*)) \pi_{\mathrm{Urb}[c]}(\boldsymbol{s}_c| \boldsymbol{s}_c^*) \pi(\boldsymbol{s}_c^*) \ \mathrm{d}\boldsymbol{s}_c^*, \label{eq:adjObsMod2}
\end{align}

for $c = 1, \ldots, C$. This formulation suggests that we can avoid sampling the true locations with an MCMC approach. Further technical details can be found in the supplementary materials section of [@altay2022accounting]. We apply the method and estimate the parameters in the model by utilizing autodifferentiation feature of Template Model Builder (TMB) package [@JSSv070i05]. 

#### Preparing inputs for estimation
We need to prepare a list of input items to be passed into the C++ script of TMB. The list can be prepared by using `prepare_input` function. In this example we use only one covariate, which is the population density. The function allows having more than one covariates as well. The file `(Nga_ppp_v2c_2015.tif)` containing the population density data can be downloaded from World Pop [@pop] web page and then converted into a raster object as follows:


```{r eval = FALSE}

r = raster::raster("Nga_ppp_v2c_2015.tif")

```

The first argument of `prepare_input` function is the response variable. This analysis is done based on a binomial response variable and accordingly we need to pass both the number of binomial trials and the number of successes for each cluster. Here the vector containing the number of binomial trials is denoted by ns and it represents the total number of 20-39 years old women in each cluster. The number of successes is passed via the vector ys and it stands for the number of 20-39 years old women who completed their secondary education in each cluster. If the response variable was Gaussian, then the list called "response" would contain only one element, which is ys, a vector of Gaussian response values.
```{r eval = FALSE}
# making a list for the binomial response
response = list(ys = nigeria.data$ys,
               ns = nigeria.data$ns)
```




<!-- # Setting the initial values : -->
<!-- intercept = logit(0.5) -->
<!-- sigma.cluster = 0 # this is the nugget variance, currently we don't have a nugget in the model -->
<!-- sigma = 1.74      # marginal variance -->
<!-- range = 114       # spatial range in kilometers -->

<!-- # Collecting these initial values into a list called "modelParams": -->
<!-- modelParams = list(intercept = intercept, -->
<!--                                range = range,   -->
<!--                                sigma = sigma, -->
<!--                                sigma.cluster = sigma.cluster) -->


Coordinates of the survey cluster centers should be in kilometers, so that the function can locate the rings and the integration points accordingly: 

```{r eval = FALSE}
locObs =cbind(nigeria.data[["east"]], nigeria.data[["north"]])  # example coordinates of the survey clusters.
                                                              # In real data set, this would correspond to the 
                                                              # DHS survey cluster centers, converted to kilometers
```
 
Setting the other arguments:
```{r eval = FALSE}
likelihood = 1 # binomial likelihood
jScale = 1 # represents the default DHS maximum jittering distances
urban = nigeria.data$urbanRuralDHS # shows the urbanicity classification of each cluster center. Takes "U" 
                               # if the cluster center is located within an urban area and takes "R" 
                               # if the cluster center is located within a rural area

# remember that we previously re-read the admin2 file to plot the borders
# and we need to remove the lake and add the object ID again:

admin2 = admin2[-160,] # removing the lake (contained in polygon 160)
admin2@data[["OBJECTID"]] =1:774 # we now include the lake which is polygon number 160

adminMap = admin2              # SpatialPolygonsDataFrame object that contains
                               # the administrative borders that are respected
                               # while jittering is done
covariateData = list(covariate1 = r)  # a list that contains covariate rasters
                                      # covariates should pass into the function as raster objects
```

Arguments that will be used while constructing the spde object:
```{r eval = FALSE}
USpatial = 1
alphaSpatial = 0.05
```

And finally constructing the data input list:
```{r eval = FALSE}
inputData = prepare_input(response=response, locObs=locObs, likelihood = likelihood, 
              jScale=jScale, urban = urban, mesh.s = mesh.s, adminMap=adminMap, nSubAPerPoint=10, nSubRPerPoint = 10,
              covariateData=covariateData)

View(data)
```
The function creates a set of integration points around each cluster center, the cluster center itself 
being the first integration point. Then the function:

* Calculates weights corresponding to each integration point
* Creates the number of observations (y_i) out of the number of exposures (n_i) separately for the rural and urban integration points
* Extracts the covariate values from each covariate raster at locations corresponding to the coordinates of the integration points
* Builds separate design matrices for urban and rural locations
* Constructs two projection matrices seperately for the urban and rural locations

The data list created by this function contains the spde components as well. The final data input list looks like the following :

```{r eval = FALSE} 
  inputData <- list(num_iUrban,  # Total number of urban observations
               num_iRural,  # Total number of rural observations
               num_s, # num. of vertices in SPDE mesh
               y_iUrban, # num. of pos. urban obs in the cluster
               y_iRural, # num. of pos. rural obs in the cluster
               n_iUrban,  # num. of urban exposures in the cluster
               n_iRural,  # num. of rural exposures in the cluster
               n_integrationPointsUrban, # num. of urban integration points
               n_integrationPointsRural, # num. of rural integration points
               wUrban = wUrban, # urban weights
               wRural = wRural, # rural weights
               X_betaUrban = desMatrixJittUrban,
               X_betaRural = desMatrixJittRural,
               M0, # = spde[['param.inla']][['M0']], # SPDE sparse matrix
               M1, # = spde[['param.inla']][['M1']], # SPDE sparse matrix
               M2, # = spde[['param.inla']][['M2']], # SPDE sparse matrix
               AprojUrban,  # Projection matrix (urban)
               AprojRural,  # Projection matrix (rural)
               options = c(1, ## if 1, use normalization trick
                           1), ## if 1, run adreport
               flag1 = 1, # normalization flag.
               flag2 = flag2, #(0/1 for Gaussian/Binomial)
  )

```

#### Estimation

We use `estimateModel` function to estimate the model parameters, namely the intercept, coefficients of the covariates, spatial range, marginal variance and if the model has a nugget, then also the nugget standard deviation. The main argument of the function is the list called "data", which we have just created above. Besides that, the function requires a list related to the priors and another list of options to decide which model components should be adjusted for jittering.

The argument "options" should have two elements, namely, "random" and "covariates". They refer to the random effect and the covariates in the model. We can adjust for jittering either in the random effect or the covariates or both. Setting these arguments individually to 1 and 0, turns the jittering adjustment on and off in the corresponding model component, respectively. It is possible to not adjust for jittering at all, by setting them both to zero as well.

<!-- If one wants to try the approach which suggests using smoothed covariate rasters, instead of using the autodifferentiation of TMB, then both arguments needs to be set to zero again. Because that method does not account for jittering in the sense that our method does.  -->

The list called "priors" contains the values that will be used to create PC priors for the spatial range and marginal variance, together with the Gaussian priors for the intercept and the coefficients of the covariates. The argument "beta" should be passed as a vector of two elements, where the first one is the mean and the second one is the standard deviation for the Gaussian prior. The second argument "range" represents the median spatial range in kilometers for the PC (Penalized-complexity) priors. We recommend choosing this value as the 10% of the longest distance (in kilometers) between the points along the borders of the corresponding country.


```{r eval = FALSE}
nNodes = mesh.s[['n']]

est = estimateModel(data = inputData, 
                    nNodes = nNodes,
                    options = list(random = 1, covariates = 1), # we want to account for jittering in both
                    priors = list(beta = c(0,1), range = 114, USpatial = 1, alphaSpatial = 0.05))

```
The `estimateModel` function creates a list of two elements as oiutput. The first element of the list is called "res" (short for results) and contains the values of the model parameter estimates. The second element of the list is called "obj". This is the product of running TMB and it is created by "MakeADFun" function of TMB. We output this from the function since it will be needed as an input for the prediction step. 

## Predictions with the Model

### Creating a prediction grid

Once the model parameters are estimated, the next step will be to make predictions at new locations using our model. In this section we will first  construct a grid of points with respect to the bounding box (bbox) of the SpatialPolygonsDataFrame which represents the borders of the country of interest. Then we will chose certain number of points from them randomly. These are done by the "gridCountry" function to create a set of prediction locations and obtain their coordinates both in degrees and in kilometers.

```{r eval = FALSE}

predCoord = gridCountry(admin0 = admin0, m = 50, n = 50)

grid = data.frame(long = predCoord[,1], lat = predCoord[,2])

pdf("~/Desktop/grid.pdf", width = 25, height = 14) # Open a new pdf file
plot(grid, asp=1)
plot(admin0, border = "red", add=TRUE)
dev.off()

head(predCoord)   # randomly chosen prediction points:

        long       lat      east     north
1  5.119040  6.823085 -3476.933  907.8185
2 13.206050 10.946625 -2411.477 1341.1260
3  7.079527  6.823085 -3220.076  888.0158
4  4.628918  7.804880 -3530.615 1043.7634
5  8.549893  5.055854 -3044.689  648.3731
6  6.344345  6.430367 -3319.562  843.8708

```

![The prediction grid](/Users/umuta/GeoAdjust/vignettes/grid.png){ width=100% }



### Predictions at the new prediction points

Previously we estimated the model parameters using "estimateModel" function and the function created an output object called "obj", besides the parameter estimates. We will now use this TMB object to make predictions at the set of prediction points that we have just created. We use "predRes" function for the predictions. The function needs the TMB object ("obj") and the coordinates of the prediction points in kilometers.

```{r eval=FALSE}

predictions = predRes(obj = est[["obj"]], predCoords = predCoords)

```

## References





