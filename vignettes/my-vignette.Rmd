---
title: "GeoAdjust"
output: rmarkdown::html_vignette
bibliography: /Users/umuta/GeoAdjust/references.bib
vignette: >
  %\VignetteIndexEntry{GeoAdjust}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style>
body {
text-align: justify}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, setup}
library(GeoAdjust)
```


## INTRODUCTION

The Demographic and Health Surveys Program (DHS) conducts household surveys in low- and middle-income countries.
DHS wants to make sure that the privacy of the survey participants is protected before the data is published. They first aggregate the data collected from the households into the GPS coordinates of each household cluster center. This step is followed by randomly displacing the true cluster centers with respect to the stratum that they belong to. The household survey design consists of two strata as urban and rural.

DHS applies jittering (small local random displacements) with respect to a known jittering algorithm. The algorithm basically involves first choosing a uniform random angle between 0 and $2\pi$ and then shifting the true location up to a certain maximum distance towards the corresponding direction. Accordingly, the urban cluster centers are displaced up to 2 kilometers while the rural ones are jittered up to 5 kilometers, with %1 of them being jittered up to 10 kilometers [@DHSspatial07]. The jittering is done by respecting the administrative area borders, meaning that the jittered location is not allowed to end up in an administrative area that is different from the initial one. Jittering is repeated until this condition is satisfied. Administrative areas are classified into levels. The first level which is eather called the "national level" or usually coded as "admin0" represents the natinal borders of the country. Then the subnational levels follows either as counties, provinces, etc., or as admin1, admin2, etc. It is important to be aware that the jittering can be done with respect to different admin levels in different countries. The information about this can be found in the corresponding README.txt (it is called "GPS_Displacement_README.txt" for the Nigeria 2018 survey, might be different elsewhere) file of the corresponding DHS data set. 

The random displacement leads the DHS coordinates to contain positional error. We recently discovered that the unaddressed positional error results in the model parameter estimates that are attenuated towards zero together with a poorer predictive performance [@altay2022accounting], [@altay2022covariates]. GeoAdjust package is constructed in order to provide useful computational tools which account for positional uncertainty in the GPS coordinates of the observed data. The package follows the methods that are developed and discussed in detail in [@altay2022accounting] and [@altay2022covariates].

### Access to DHS Data

DHS data sets are not openly available to the public. Access to relevant DHS survey data sets requires applying for permission. One needs to briefly and clearly explain why the data is needed and how it will be processed and used. Sharing the data sets with the collaborating researchers requires permission as well. Step by step explanation about the application process can be found in DHS website. Once the permission is granted, the applicant receives a letter via email, which clearly states the content of the permission.


<!-- The methods included in GeoAdjust package utilize the auto-differentiation in Template Model Builder (TMB) R package. The approach involves creating a set of integration points within equidistant rings around each cluster center. Each cluster center is taken as the first integration point of the corresponding point set, and then depending on the urbanicity of the cluster, different number of rings are created together with their own sets of integration points. This approach allows us to consider the unknown true locations as the nuisance parameters in the model and then integrate them out by utilizing the auto differentiation of the Template Model builder (TMB) R-package. TMB operates via a C++ script which  contains the joint negative log-likelihood function and allows computing the contribution of each integration point to the joint negative log-likelihood. -->

<!-- This package aims to provide fast and useful tools that allow doing inference and prediction while adjusting/accounting for the positional error in the GPS coordinates of the survey cluster centers. -->

## THE MODEL
Assume that we want to estimate the risk of completing secondary education among 20-39 years old women throughout Nigeria. We call the true GPS coordinates of the centroids of each household cluster as the "true locations", whereas the published GPS coordinates of the cluster centers are referred as the "observed locations". These locations contain positional error due to being jittered. We need to adjust for this, in order to avoid its adverse impact on inference.

Let $\boldsymbol{s}_1^*, \ldots, \boldsymbol{s}_C^* \in\mathcal{D}$ denote the true locations of the corresponding published DHS cluster centers $\boldsymbol{s}_1, \ldots, \boldsymbol{s}_C$. Adjusting for jittering yields the following observation model:
\begin{align}
\label{eqn:adjObsModel}
\begin{split}
y_c | r_c,n_c &\sim \text{Binomial}(n_c, r_c), \quad \boldsymbol{s}_c|\boldsymbol{s}_c^*\sim \pi_{\mathrm{Urb}[c]}(\boldsymbol{s}_c|\boldsymbol{s}_c^*),\\
    r_c | \boldsymbol{s}_c^* &= r(\boldsymbol{s}_c^*) = \mathrm{logit}^{-1}( \eta(\boldsymbol{s}_c^*)),
\end{split}
\end{align}

where $r_c$ denotes the risk of interest in cluster $c$, and $\mathrm{Urb}[c]\in\{\mathrm{U}, \mathrm{R}\}$ corresponds to the urban (U) or rural (R) stratum that cluster c is located within. 

In this observation model, both $y_c$ and $\boldsymbol{s}_c$ are already available through the DHS data set and accordingly, treated as the observed quantities. The unobserved true locations $s_c^*$ on the other hand, are inaccessible. They are treated as the random quantities and assigned a uniform prior $s_c^*\sim \mathcal{U}(\mathcal{D})$. This implies that we treat all locations $\boldsymbol{s}_c^*$ within the maximum jittering distance from $\boldsymbol{s}_c$ as equally likely \emph{a priori}, meaning that the true location can be located anywhere within the corresponding jittering radius with equal probability.

## DATA PREPARATION
The survey data consists of both individual level and cluster level information. The individual level data is a Stata file that mainly contains the responses of the participants to the survey. The file has a variable indicating the corresponding cluster ID of each data row. The cluster level data is a shape file. It contains cluster identification information such as their jittered latitudes and longitudes (in degrees), cluster ID, corresponding strata ("R" for rural and "U" for urban) and the name of the administrative area that they are located within. Since the cluster ID is a common variable in both files, it will be useful while merging them into a single data frame. 

It is not possible to share DHS data publicly, therefore the real survey data will not be processed through this vignette. Instead, an example data set will be used. Similar to the Nigeria DHS 2018 household survey (NDHS-2018) data files, the example data set consist of a Stata file and a shape file.

Additionally, two more shape files are needed. One of them is the shape file that contains the national (admin0) level borders of the country of interest. The second one should contain the borders that correspond to the administrative level areas with respect to which the true cluster centers are jittered. Shape files of various administrative levels for different countries are available at the  website of "the Database of Global Administrative Areas (GADM)" (https://gadm.org/data.html). Once downloaded, the files can be read into R as "SpatialPolygonsDataFrame" objects.

Assume that we want to estimate the risk of completing secondary education among 20-39 years old women throughout Nigeria. Following is the demonstration for first extracting and then merging the individual and cluster level data, based on the Nigeria DHS 2018 (NDHS-2018) household survey file and variable names:

### Reading the shape files of administrative area borders:

```{r eval=FALSE}

# find the path to the example files:
system.file("extdata", "individualLevel.DTA", package = "GeoAdjust", mustWork = TRUE)
[1] "path1/individualLevel.DTA"

system.file("extdata", "clusterLevel", package = "GeoAdjust", mustWork = TRUE)
[1] "path2/clusterLevel"                # path2 is the path of the folder that
                                        # contains the .shp file

# the shape files corresponding to the admin0 and admin2 borders :
library(rgdal)
admin0 = readOGR(dsn = "dataFiles/gadm40_NGA_shp",
                            layer = "gadm40_NGA_0")

admin2 = readOGR(dsn = "dataFiles/gadm40_NGA_shp",
                     layer = "gadm40_NGA_2")

admin2 = admin2[-160,] # removing the lake (contained in polygon 160)
```

### Reading the DHS files:

```{r eval = FALSE}
# individual level data (survey responses) :
educationData = read_dta("path1") 

# cluster level data (clusterID, cluster center coordinates, strata, etc.)
corList = readOGR(dsn = "path2", 
                             layer = "clusterLevel")
```


### Extracting data:

```{r eval = FALSE}
# extract cluster level information:
smallGeo = data.frame(clusterIdx = corList$DHSCLUST, urban = corList$URBAN_RURA,
                      long = as.vector(corList@coords[,1]), lat = as.vector(corList@coords[,2]),
                      admin1 = corList$ADM1NAME)

#  extract individual level information:
myData = data.frame(clusterIdx = educationData$v001,         # ID numbers of the clusters
                    age = educationData$v012,                # age of the individual
                    secondaryEducation = educationData$v106) # v106 : highest education level attended
                                                             #    0 : no education
                                                             #    1 : primary
                                                             #    2 : secondary
                                                             #   >2 : higher
```
In order to be consistent with NDHS-2018 survey, the example data is constructed using the same variable names (v001, v012, etc.). Detailed explanation about variables are found in [@fund2018demographic].

### Subsetting and merging data:
```{r eval = FALSE}
# subset data with respect to the age interval of interest:
myData = subset(myData, age <= 39 & age >=20)

# number of 20-39 years old women who completed secondary education in each household
myData$ys = (myData$secondaryEducation>=2)+0

# merge the cluster level data with the subsetted individual level data,
# with respect to the cluster ID:
myData = merge(myData, smallGeo, by = "clusterIdx")

# add number of trials (for binomial response)
myData$Ntrials = 1

# aggregate the survey responses to the cluster centers
answers_x = aggregate(myData$ys,
                      by = list(clusterID = myData[, 1]),
                      FUN = sum)

answers_n= aggregate(myData$ys,
                     by = list(clusterID = myData[, 1]),
                     FUN = length)

# merge
answers_joint = merge(answers_x, answers_n, by="clusterID")

# now we have the total number of women participants within the relevant age interval (ns), 
# for each cluster. We also have the number of women among those who completed their secondary education (ys)
colnames(answers_joint) = c("clusterID", "ys", "ns")
```

### Collecting everything into a main data frame:
```{r eval = FALSE}
# initial data frame
nigeria.data = data.frame(clusterID = corList@data[["DHSCLUST"]], long = as.vector(corList@coords[,1]), lat = as.vector(corList@coords[,2]))

# add ys and ns
nigeria.data = merge(nigeria.data, answers_joint, by="clusterID", all=T)

# add strata:
nigeria.data$urbanRuralDHS = corList@data[["URBAN_RURA"]]

# add coordinates in kilometers
nigeria.data$east = rep(NA, length(nigeria.data$long))
nigeria.data$north = rep(NA, length(nigeria.data$long)) 

nigeria.data[,c("east", "north")] = convertDegToKM(nigeria.data[,c("long", "lat")])
```

### Fine tuning:
```{r eval = FALSE}
# jittering is done by respecting admin2 borders in Nigeria. 
# see if there are  cluster centers that doesn't match with any of the admin2 areas:

# first, add polygon IDs (some shape files may have it already) :
admin2@data[["OBJECTID"]] =1:774 # normally 775, we removed one (the lake)
                                 
# the cluster coordinates:
latLon = cbind(nigeria.data[,"long"], nigeria.data[,"lat"])
colnames(latLon) = c("long", "lat")

# make a SpatialPoints object
latLon = SpatialPoints(latLon, proj4string=CRS("+proj=longlat +datum=WGS84 +no_defs"), bbox = NULL)

# see if the points (cluster centers) are within the polygons (admin2 areas) :
check1 <- over(latLon, admin2, returnList = FALSE)

# drop the rows which don't match with none of the admin2 areas. 
# we will need them to match while creating the integration points later on.

# see which ones don't return a match :
which(is.na(check1$NAME_2))
# integer(0)


# This might not always be the case. If there were some non-matching rows,
# then they would be dropped as follows:

# which(is.na(check1$NAME_2))  # see the rows that don't match:
# [1]   48  122  205  848  857 1116 1122 1287 1328

# drop the corresponding rows from the main data set :
#nigeria.data = nigeria.data[-c(48, 122,  205,  848,  857, 1116, 1122, 1287, 1328),]
```

At this point the data frame "nigeria.data" should contain the variables that will be needed further in the analysis. 



### The SPDE Approach

We model the latent spatial part $u(\cdot)$ of the observation model as a Gaussian Random Field with a Matern covariance function. The stochastic partial differential equations (SPDE) approach [@Lindgren:etal:11] provides an approximation to the Matérn GRF that results in a sparse precision matrix. This is applied by first triangulating the area of interest through $m$ nodes and then approximating the GRF $u(\cdot)$ by:

\begin{equation}
\tilde{u}(\boldsymbol{s}) = \sum_{i=1}^m w_i \phi_i(s),\label{eq:SPDE:basis}
\end{equation}
where $\phi_i(\cdot)$ are pyramidal basis functions and  $\boldsymbol{w} = (w_1 \ \ldots \ w_m)^\mathrm{T}$
are weights for the basis functions. The SPDE approach results in a distribution $\boldsymbol{w}\sim\mathcal{N}_m(\boldsymbol{0}, \mathbf{Q}(\boldsymbol{\theta})^{-1})$, where $\mathbf{Q}(\boldsymbol{\theta})$ is sparse.

#### Triangulation - The Mesh

We will construct the triangulation mesh based on the admin0 borders (in form of a SpatialPolygonsDataFrame object) of the country of interest. It is possible to manipulate the mesh resolution by setting the arguments max.edge and offset accordingly. 

```{r eval=FALSE}
mesh.s = meshCountry(admin0 = admin0, max.edge = c(25, 50), offset=-.08)

# It might be a good idea to plot the mesh and the country borders together:

pdf("~/Desktop/mesh.pdf", width = 25, height = 14) # Open a new pdf file
plot(mesh.s)
plot(admin0, border = "red", add=TRUE)
dev.off() 

```

To be able to estimate the model parameters, first we need to prepare a list of items that will be required as input by the Template Model Builder (TMB). We use "prepare_input" function to prepare the mentioned list. 

#### Preparing inputs for estimation
```{r eval = FALSE}
# the example data set :

# We will use only the population density as the covariate in the model. This can be downloaded
# as a .tif file from WorldPop and then converted into a raster as follows:

library(wpgpDownloadR)
r <- wpgpGetCountryDataset(ISO3 = "NGA",
                               covariate = "ppp_2015",
                               destDir ="the folder destination path goes here")

r = raster::raster("nga_ppp_2015.tif")
plot(r)

# making a list for the binomial response
response = list(ys = countryData$ys,
               ns = countryData$ns)

# Setting the initial values :
intercept = logit(0.5)
sigma.cluster = 0 # this is the nugget variance, currently we don't have a nugget in the model
sigma = 1.74      # marginal variance
range = 114       # spatial range in kilometers


# Collecting these initial values into a list called "modelParams":
modelParams = list(intercept = intercept,
                               range = range,  
                               sigma = sigma,
                               sigma.cluster = sigma.cluster)


# coordinates of the example survey cluster centers in kilometers

locObs =cbind(countryData[["east"]], countryData[["north"]])  # example coordinates of the survey clusters
                                                              # in real data set, this would correspond to the 
                                                              # DHS survey cluster centers, converted to kilometers
                       
# Setting the other arguments                                         
likelihood = 1 # binomial likelihood
jScale = 1 # represents the default DHS maximum jittering distances
urban = countryData$urbanRural # shows the urbanicity classification of each cluster center. Takes "U" 
                               # if the cluster center is located within an urban area and takes "R" 
                               # if the cluster center is located within a rural area



adminMap = admin2
covariateData = r 

# arguments that will be used while constructing the Penalized Complexity (PC) priors
USpatial = 1
alphaSpatial = 0.05

# and finally constructing the data input list:
Data = prepareData(response=response, locObs=locObs, modelParams=modelParams, likelihood = likelihood, 
              jScale=jScale, urban = urban, mesh.s = mesh.s, adminMap=adminMap, nSubAPerPoint=10, 
              nSubRPerPoint=10, testMode=FALSE, covariateData=covariateData)
```
The function creates a set of integration points around each cluster center, the cluster center itself 
being the first integration point. Then the function:

* Calculates weights corresponding to each integration point, 
* Creates the number of observations (y_i) out of the number of exposures ((y_i (the number of successes) out of (n_i) separately for the rural and urban integration points,
* Extracts the covariate values from each covariate raster at locations corresponding to the coordinates of the integration points 
* Builds separate design matrices for urban and rural locations
* Constructs two projection matrices seperately for the urban and rural locations. 

The data list created by this function contains the spde components as well. The final data input list looks like the following :

```{r eval = FALSE} 
  data <- list(num_iUrban,  # Total number of urban observations
               num_iRural,  # Total number of rural observations
               num_s, # num. of vertices in SPDE mesh
               y_iUrban, # num. of pos. urban obs in the cluster
               y_iRural, # num. of pos. rural obs in the cluster
               n_iUrban,  # num. of urban exposures in the cluster
               n_iRural,  # num. of rural exposures in the cluster
               n_integrationPointsUrban, # num. of urban integration points
               n_integrationPointsRural, # num. of rural integration points
               wUrban = wUrban, # urban weights
               wRural = wRural, # rural weights
               X_betaUrban = desMatrixJittUrban,
               X_betaRural = desMatrixJittRural,
               M0, # = spde[['param.inla']][['M0']], # SPDE sparse matrix
               M1, # = spde[['param.inla']][['M1']], # SPDE sparse matrix
               M2, # = spde[['param.inla']][['M2']], # SPDE sparse matrix
               AprojUrban,  # Projection matrix (urban)
               AprojRural,  # Projection matrix (rural)
               options = c(1, ## if 1, use normalization trick
                           1), ## if 1, run adreport
               flag1 = 1, # normalization flag.
               flag2 = flag2, #(0/1 for Gaussian/Binomial)
  )

```

## ESTIMATING THE MODEL PARAMETERS

We use "estimateModel" function to estimate the model parameters, namely the intercept, coefficients of the covariates, spatial range, marginal variance and if the model has a nugget, then also the nugget standard deviation. The main input that the function needs is the list called "data", which we have just created above. Besides that, the function requires a list related to the priors and another list of options to decide which model components should be adjusted for jittering.

The list "options" should have two arguments, namely, "random" and "covariates", each referring to the corresponding model component. We turn the option of accounting for jittering on and off by setting each one of those argument to either 1 or zero, respectively. It is also possible to not adjust for jittering at all, by setting both to zero. 

If one wants to try the approach which suggests using smoothed covariate rasters, instead of using the autodifferentiation of TMB, then both arguments needs to be set to zero again. Because that method does not account for jittering in the sense that our method does. 

The list called "priors" contains the values that will be used to create PC priors for the spatial range and marginal variance and the Gaussian priors for the intercept and the coefficients of the covariates. The argument "beta" should be a vector of two values, where the first one is the mean and the second one is the standard deviation for the Gaussian prior. The second argument "range" represents the median spatial range in kilometers for the PC (Penalized-complexity) priors. We recommend choosing this value as the 10% of the longest distance (in kilometers) between the points along the borders of the corresponding country.


```{r eval = FALSE}
est = estimateModel(data = data, 
                    options = list(random = 1, covariates = 1), # we want to account for jittering in both
                    priors = list(beta = c(0,1), range = 114))

# The "estimateModel" function creates a list of two elements. The first element of the list is called "res" (short for results) and contains the values of the model parameter estimates. The second element of the list is called "obj". This is the product of running TMB and it is created by "MakeADFun" function of TMB. We output this from the function since it will be needed as an input for the prediction step. 
```

## PREDICTIONS WITH THE MODEL

### Creating prediction points from a grid 

Once the model parameters are estimated, the next step will be to make predictions at new locations using our model. In this section we will first show how to construct a grid of points with respect to the bounding box (bbox) of the SpatialPolygonsDataFrame which represents the borders of the country of interest. Then we will chose certain number of points from them randomly. These are done by the "gridCountry" function to create a set of prediction locations and obtain their coordinates both in degrees and in kilometers.

```{r eval = FALSE}

predCoord = gridCountry(admin0 = NULL, m = NULL, n = NULL, nPred = NULL)

```

### Predictions at the new prediction points

Previously we estimated the model parameters using "estimateModel" function and the function created an output object called "obj", besides the parameter estimates. We will now use this TMB object to make predictions at the set of prediction points that we have just created. We use "predRes" function for the predictions. The function needs the TMB object ("obj") and the coordinates of the prediction points in kilometers.

```{r eval=FALSE}

predictions = predRes(obj = est[["obj"]], predCoords = predCoords)

```







